{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c046426-96ac-455f-946f-e89b9afc4121",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import pennylane as qml\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from collections import namedtuple\n",
    "import os\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from torch.nn.parameter import Parameter\n",
    "#from agent import Agent  # Import the Agent class from agent.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084eca89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02f96d6c-d730-40bd-bb49-996b6494642f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Representa una transición en el entorno, con estado, acción, recompensa, \n",
    "# si el episodio ha terminado (done), y el siguiente estado.\n",
    "Transition = namedtuple('Transition', ('state', 'action', 'reward', 'done', 'next_state'))\n",
    "\n",
    "# Inicializa el buffer de memoria para almacenar las transiciones.\n",
    "def init_replay_memory(buffer_size):\n",
    "    return {\n",
    "        \"buffer_size\": buffer_size,  # Tamaño máximo del buffer\n",
    "        \"memory\": [],               # Lista para almacenar las transiciones\n",
    "        \"position\": 0               # Índice de la posición actual para insertar\n",
    "    }\n",
    "\n",
    "# Agrega una transición al buffer.\n",
    "def push_replay_memory(replay_memory, *args):\n",
    "    # Si la memoria aún no está llena, agrega un nuevo espacio\n",
    "    if len(replay_memory[\"memory\"]) < replay_memory[\"buffer_size\"]:\n",
    "        replay_memory[\"memory\"].append(None)\n",
    "    \n",
    "    # Sobrescribe la posición actual con la nueva transición\n",
    "    replay_memory[\"memory\"][replay_memory[\"position\"]] = Transition(*args)\n",
    "    # Actualiza la posición de manera circular\n",
    "    replay_memory[\"position\"] = (replay_memory[\"position\"] + 1) % replay_memory[\"buffer_size\"]\n",
    "\n",
    "# Muestra una muestra aleatoria del buffer.\n",
    "def sample_replay_memory(replay_memory, batch_size, device):\n",
    "    # Selecciona índices aleatorios sin repetición\n",
    "    indices = np.random.choice(len(replay_memory[\"memory\"]), batch_size, replace=False)\n",
    "    \n",
    "    # Extrae las transiciones correspondientes\n",
    "    states, actions, rewards, dones, next_states = zip(\n",
    "        *[replay_memory[\"memory\"][idx] for idx in indices]\n",
    "    )\n",
    "    \n",
    "    # Convierte los datos a tensores con los tipos adecuados\n",
    "    states = np.array(states) \n",
    "    states = torch.tensor(states, dtype=torch.float32, device=device)\n",
    "    actions = torch.tensor(actions, dtype=torch.long, device=device)  # Se asume que las acciones son discretas\n",
    "    rewards = torch.tensor(rewards, dtype=torch.float32, device=device)\n",
    "    dones = torch.tensor(dones, dtype=torch.bool, device=device)      # Se usa tipo booleano para \"dones\"\n",
    "    next_states = torch.tensor(next_states, dtype=torch.float32, device=device)\n",
    "\n",
    "    return states, actions, rewards, dones, next_states\n",
    "\n",
    "# Obtiene la longitud del buffer de memoria.\n",
    "def replay_memory_length(replay_memory):\n",
    "    return len(replay_memory[\"memory\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1f6e541-c409-4636-aeec-a82e7798b749",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializa los parámetros del agente y devuelve un diccionario que los representa.\n",
    "def init_agent(net, action_space, exploration_initial_eps, exploration_decay, exploration_final_eps):\n",
    "    return {\n",
    "        \"net\": net,                          # Red neuronal para calcular las acciones (Q-values)\n",
    "        \"action_space\": action_space,        # Espacio de acciones disponible\n",
    "        \"exploration_initial_eps\": exploration_initial_eps,  # Epsilon inicial para la exploración\n",
    "        \"exploration_decay\": exploration_decay,              # Decadencia del epsilon\n",
    "        \"exploration_final_eps\": exploration_final_eps,      # Epsilon mínimo final\n",
    "        \"epsilon\": 0.                        # Epsilon actual (se actualiza en cada paso)\n",
    "    }\n",
    "\n",
    "# Devuelve una acción según el estado actual y la política del agente.\n",
    "def agent_call(agent, state, device=torch.device('cpu')):\n",
    "    # Explora aleatoriamente con probabilidad epsilon o elige acción basada en Q-values.\n",
    "    if np.random.random() < agent[\"epsilon\"]:\n",
    "        action = get_random_action(agent)\n",
    "    else:\n",
    "        action = get_action(agent, state, device)\n",
    "    return action\n",
    "\n",
    "# Genera una acción aleatoria del espacio de acciones.\n",
    "def get_random_action(agent):\n",
    "    action = agent[\"action_space\"].sample()  # Muestra una acción aleatoria del espacio\n",
    "    return action\n",
    "\n",
    "# Calcula la acción óptima según los Q-values de la red neuronal.\n",
    "def get_action(agent, state, device=torch.device('cpu')):\n",
    "    # Convierte el estado a tensor si no lo es ya.\n",
    "    if not isinstance(state, torch.Tensor):\n",
    "        state = torch.tensor([state], dtype=torch.float32)  # Especifica el tipo de datos para el tensor\n",
    "\n",
    "    state = state.to(device)  # Asegura que el estado esté en el dispositivo correcto (CPU/GPU)\n",
    "\n",
    "    # Evalúa los Q-values usando la red neuronal\n",
    "    q_values = agent[\"net\"].eval()(state)\n",
    "    _, action = torch.max(q_values, dim=1)  # Selecciona la acción con el mayor Q-value\n",
    "    return int(action.item())\n",
    "\n",
    "# Actualiza el valor de epsilon según el paso actual.\n",
    "def update_epsilon(agent, step):\n",
    "    agent[\"epsilon\"] = max(\n",
    "        agent[\"exploration_final_eps\"],\n",
    "        agent[\"exploration_final_eps\"] +\n",
    "        (agent[\"exploration_initial_eps\"] - agent[\"exploration_final_eps\"]) *\n",
    "        agent[\"exploration_decay\"]**step\n",
    "    )\n",
    "    return agent[\"epsilon\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e68de12-3de4-4940-b926-b046b66e054f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializa los parámetros del entrenador en un diccionario\n",
    "def init_trainer(env, net, target_net, gamma, learning_rate, batch_size,\n",
    "                 exploration_initial_eps, exploration_decay, exploration_final_eps,\n",
    "                 train_freq, target_update_interval, buffer_size, learning_rate_input=None,\n",
    "                 learning_rate_output=None, loss_func='MSE', optim_class='RMSprop',\n",
    "                 device='cpu', logging=False):\n",
    "    \n",
    "    assert loss_func in ['MSE', 'L1', 'SmoothL1'], \"Supported losses : ['MSE', 'L1', 'SmoothL1']\"\n",
    "    assert optim_class in ['SGD', 'RMSprop', 'Adam', 'Adagrad', 'Adadelta'], \\\n",
    "        \"Supported optimizers : ['SGD', 'RMSprop', 'Adam', 'Adagrad', 'Adadelta']\"\n",
    "    assert device in ['auto', 'cpu', 'cuda:0'], \"Supported devices : ['auto', 'cpu', 'cuda:0']\"\n",
    "\n",
    "    # Configura el dispositivo\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() and device == \"auto\" else device)\n",
    "    net = net.to(device)\n",
    "    target_net = target_net.to(device)\n",
    "\n",
    "    # Configura la función de pérdida\n",
    "    loss_func = getattr(nn, loss_func + 'Loss')()\n",
    "\n",
    "    # Configura el optimizador\n",
    "    optim_class = getattr(optim, optim_class)\n",
    "    params = [{'params': net.q_layers.parameters()}]\n",
    "    if hasattr(net, 'w_input') and net.w_input is not None:\n",
    "        params.append({'params': net.w_input, 'lr': learning_rate_input or learning_rate})\n",
    "    if hasattr(net, 'w_output') and net.w_output is not None:\n",
    "        params.append({'params': net.w_output, 'lr': learning_rate_output or learning_rate})\n",
    "    opt = optim_class(params, lr=learning_rate)\n",
    "\n",
    "    # Inicializa la memoria de repetición\n",
    "    memory = init_replay_memory(buffer_size)\n",
    "\n",
    "    # Inicializa el agente\n",
    "    agent = init_agent(net, env.action_space, exploration_initial_eps, exploration_decay, exploration_final_eps)\n",
    "\n",
    "    # Configura el registro de logs\n",
    "    log_dir = None\n",
    "    writer = None\n",
    "    if logging:\n",
    "        exp_name = datetime.now().strftime(\"DQN-%d_%m_%Y-%H_%M_%S\")\n",
    "        log_dir = os.path.join('./logs/', exp_name)\n",
    "        os.makedirs(log_dir, exist_ok=True)\n",
    "        writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    return {\n",
    "        \"env\": env,\n",
    "        \"net\": net,\n",
    "        \"target_net\": target_net,\n",
    "        \"gamma\": gamma,\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"train_freq\": train_freq,\n",
    "        \"target_update_interval\": target_update_interval,\n",
    "        \"memory\": memory,\n",
    "        \"agent\": agent,\n",
    "        \"opt\": opt,\n",
    "        \"loss_func\": loss_func,\n",
    "        \"device\": device,\n",
    "        \"logging\": logging,\n",
    "        \"log_dir\": log_dir,\n",
    "        \"writer\": writer,\n",
    "        \"global_step\": 0,\n",
    "        \"episode_count\": 0\n",
    "    }\n",
    "\n",
    "# Reinicia el entrenador para un nuevo entrenamiento\n",
    "def reset_trainer(trainer):\n",
    "    trainer[\"global_step\"] = 0\n",
    "    trainer[\"episode_count\"] = 0\n",
    "    trainer[\"n_actions\"] = trainer[\"env\"].action_space.n\n",
    "\n",
    "    # Llena el buffer de memoria\n",
    "    state, _ = trainer[\"env\"].reset(seed=123)\n",
    "    while replay_memory_length(trainer[\"memory\"]) < trainer[\"memory\"][\"buffer_size\"]:\n",
    "        action = get_random_action(trainer[\"agent\"])\n",
    "        next_state, reward, terminated, truncated, _ = trainer[\"env\"].step(action)\n",
    "        done = terminated or truncated\n",
    "        push_replay_memory(trainer[\"memory\"], state, action, reward, done, next_state)\n",
    "        state = next_state if not done else trainer[\"env\"].reset(seed=123)[0]\n",
    "\n",
    "# Actualiza la red neuronal principal\n",
    "def update_net(trainer):\n",
    "    trainer[\"net\"].train()\n",
    "    trainer[\"opt\"].zero_grad()\n",
    "\n",
    "    # Muestra de memoria\n",
    "    states, actions, rewards, dones, next_states = sample_replay_memory(\n",
    "        trainer[\"memory\"], trainer[\"batch_size\"], trainer[\"device\"]\n",
    "    )\n",
    "\n",
    "    # Q-values actuales\n",
    "    state_action_values = trainer[\"net\"](states).gather(1, actions.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "    # Q-values objetivo\n",
    "    with torch.no_grad():\n",
    "        next_state_values = trainer[\"target_net\"](next_states).max(1)[0].detach()\n",
    "    expected_state_action_values = rewards + trainer[\"gamma\"] * (1 - dones.float()) * next_state_values\n",
    "\n",
    "    # Calcula la pérdida\n",
    "    loss = trainer[\"loss_func\"](state_action_values, expected_state_action_values)\n",
    "    loss.backward()\n",
    "    trainer[\"opt\"].step()\n",
    "\n",
    "    return loss.item()\n",
    "\n",
    "# Copia los parámetros de la red principal a la red objetivo\n",
    "def update_target_net(trainer):\n",
    "    trainer[\"target_net\"].load_state_dict(trainer[\"net\"].state_dict())\n",
    "\n",
    "# Ejecuta un paso de entrenamiento\n",
    "def train_step(trainer):\n",
    "    episode_epsilon = update_epsilon(trainer[\"agent\"], trainer[\"episode_count\"])\n",
    "    episode_steps, episode_reward, episode_loss = 0, 0, []\n",
    "\n",
    "    state, _ = trainer[\"env\"].reset(seed=123)\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        action = agent_call(trainer[\"agent\"], state, trainer[\"device\"])\n",
    "        next_state, reward, terminated, truncated, _ = trainer[\"env\"].step(action)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        # Actualiza la memoria\n",
    "        push_replay_memory(trainer[\"memory\"], state, action, reward, done, next_state)\n",
    "        state = next_state\n",
    "\n",
    "        # Actualiza la red\n",
    "        if trainer[\"global_step\"] % trainer[\"train_freq\"] == 0:\n",
    "            loss = update_net(trainer)\n",
    "            episode_loss.append(loss)\n",
    "\n",
    "        # Actualiza la red objetivo\n",
    "        if trainer[\"global_step\"] % trainer[\"target_update_interval\"] == 0:\n",
    "            update_target_net(trainer)\n",
    "\n",
    "        trainer[\"global_step\"] += 1\n",
    "        episode_reward += reward\n",
    "        episode_steps += 1\n",
    "\n",
    "    trainer[\"episode_count\"] += 1\n",
    "    return {\n",
    "        'steps': episode_steps,\n",
    "        'loss': np.mean(episode_loss) if episode_loss else 0.,\n",
    "        'reward': episode_reward,\n",
    "        'epsilon': episode_epsilon\n",
    "    }\n",
    "\n",
    "# Realiza un paso de evaluación\n",
    "def test_step(trainer, n_eval_episodes):\n",
    "    episode_steps = []\n",
    "    episode_rewards = []\n",
    "\n",
    "    for _ in range(n_eval_episodes):\n",
    "        state, _ = trainer[\"env\"].reset(seed=123)  # Reinicia el entorno\n",
    "        done = False\n",
    "        episode_steps.append(0)\n",
    "        episode_rewards.append(0)\n",
    "\n",
    "        while not done:\n",
    "            # Selecciona la mejor acción basada en la red (sin exploración)\n",
    "            action = get_action(trainer[\"agent\"], state, trainer[\"device\"])\n",
    "            next_state, reward, terminated, truncated, _ = trainer[\"env\"].step(action)\n",
    "            done = terminated or truncated\n",
    "\n",
    "            # Actualiza los valores acumulados\n",
    "            state = next_state\n",
    "            episode_steps[-1] += 1\n",
    "            episode_rewards[-1] += reward\n",
    "\n",
    "    # Calcula promedios\n",
    "    avg_steps = np.mean(episode_steps)\n",
    "    avg_reward = np.mean(episode_rewards)\n",
    "\n",
    "    return {'steps': avg_steps, 'reward': avg_reward}\n",
    "\n",
    "# Registra los resultados del entrenamiento y evaluación\n",
    "def log_training(trainer, train_stats, test_stats, episode, log_train_freq, log_ckp_freq):\n",
    "    if trainer[\"logging\"]:\n",
    "        writer = trainer[\"writer\"]\n",
    "\n",
    "        # Registra las estadísticas de entrenamiento\n",
    "        if episode % log_train_freq == 0:\n",
    "            for key, value in train_stats.items():\n",
    "                writer.add_scalar(f\"train/{key}\", value, episode)\n",
    "\n",
    "        # Registra las estadísticas de evaluación\n",
    "        if test_stats and (episode % log_train_freq == 0):\n",
    "            for key, value in test_stats.items():\n",
    "                writer.add_scalar(f\"test/{key}\", value, episode)\n",
    "\n",
    "        # Guarda los pesos del modelo\n",
    "        if log_ckp_freq > 0 and episode % log_ckp_freq == 0:\n",
    "            checkpoint_path = os.path.join(trainer[\"log_dir\"], f\"episode_{episode}.pt\")\n",
    "            torch.save(trainer[\"net\"].state_dict(), checkpoint_path)\n",
    "\n",
    "# Ejecuta el entrenamiento del modelo por un número total de episodios\n",
    "def train(trainer, total_episodes, n_eval_episodes=5, log_train_freq=-1, log_eval_freq=-1, log_ckp_freq=-1):\n",
    "    # Inicializa las estadísticas\n",
    "    postfix_stats = {}\n",
    "    with tqdm(range(total_episodes), desc=\"DQN Training\", unit=\"episode\") as episodes:\n",
    "\n",
    "        for episode in episodes:\n",
    "            # Paso de entrenamiento\n",
    "            train_stats = train_step(trainer)\n",
    "            postfix_stats['train/reward'] = train_stats['reward']\n",
    "            postfix_stats['train/steps'] = train_stats['steps']\n",
    "\n",
    "            # Evaluación periódica\n",
    "            test_stats = None\n",
    "            if episode % log_eval_freq == 0:\n",
    "                test_stats = test_step(trainer, n_eval_episodes)\n",
    "                postfix_stats['test/reward'] = test_stats['reward']\n",
    "                postfix_stats['test/steps'] = test_stats['steps']\n",
    "\n",
    "            # Registro de métricas\n",
    "            log_training(trainer, train_stats, test_stats, episode, log_train_freq, log_ckp_freq)\n",
    "\n",
    "            # Actualiza la barra de progreso\n",
    "            episodes.set_postfix(postfix_stats)\n",
    "\n",
    "        # Guardar el modelo final\n",
    "        if trainer[\"logging\"] and log_ckp_freq > 0:\n",
    "            final_model_path = os.path.join(trainer[\"log_dir\"], \"final_model.pt\")\n",
    "            torch.save(trainer[\"net\"].state_dict(), final_model_path)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "435c3c02-6715-49fd-80b5-b94e510fdb0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Codifica las entradas en los qubits mediante puertas RX\n",
    "def encode(n_qubits, inputs):\n",
    "    for wire in range(n_qubits):\n",
    "        qml.RX(inputs[wire], wires=wire)\n",
    "\n",
    "# Define una capa de operaciones cuánticas con pesos en RY y RZ\n",
    "def layer(n_qubits, y_weights, z_weights):\n",
    "    for wire, y_weight in enumerate(y_weights):\n",
    "        qml.RY(y_weight, wires=wire)\n",
    "    for wire, z_weight in enumerate(z_weights):\n",
    "        qml.RZ(z_weight, wires=wire)\n",
    "    for wire in range(n_qubits):\n",
    "        qml.CZ(wires=[wire, (wire + 1) % n_qubits])\n",
    "\n",
    "# Define las mediciones que se realizan en el circuito\n",
    "def measure(n_qubits):\n",
    "    return [\n",
    "        qml.expval(qml.PauliZ(0) @ qml.PauliZ(1)),\n",
    "        qml.expval(qml.PauliZ(2) @ qml.PauliZ(3))\n",
    "    ]\n",
    "\n",
    "# Construye el modelo cuántico usando PennyLane y lo envuelve en un TorchLayer\n",
    "def get_model(n_qubits, n_layers, data_reupload):\n",
    "    dev = qml.device(\"default.qubit\", wires=n_qubits)\n",
    "    \n",
    "    # Define las formas de los parámetros de las capas\n",
    "    shapes = {\n",
    "        \"y_weights\": (n_layers, n_qubits),\n",
    "        \"z_weights\": (n_layers, n_qubits)\n",
    "    }\n",
    "\n",
    "    # Define el circuito cuántico\n",
    "    @qml.qnode(dev, interface='torch')\n",
    "    def circuit(inputs, y_weights, z_weights):\n",
    "        for layer_idx in range(n_layers):\n",
    "            if (layer_idx == 0) or data_reupload:\n",
    "                encode(n_qubits, inputs)\n",
    "            layer(n_qubits, y_weights[layer_idx], z_weights[layer_idx])\n",
    "        return measure(n_qubits)\n",
    "\n",
    "    # Envuelve el circuito en un TorchLayer\n",
    "    model = qml.qnn.TorchLayer(circuit, shapes)\n",
    "    return model\n",
    "\n",
    "# Inicializa los parámetros del QuantumNet\n",
    "def init_quantum_net(n_layers, w_input, w_output, data_reupload):\n",
    "    n_qubits = 4  # Número de qubits\n",
    "    n_actions = 2  # Número de acciones\n",
    "\n",
    "    # Obtiene el modelo cuántico\n",
    "    q_layers = get_model(n_qubits=n_qubits, n_layers=n_layers, data_reupload=data_reupload)\n",
    "\n",
    "    # Inicializa los parámetros de entrada y salida si se especifica\n",
    "    w_input_param = None\n",
    "    if w_input:\n",
    "        w_input_param = torch.empty(n_qubits)\n",
    "        nn.init.normal_(w_input_param, mean=0.)\n",
    "\n",
    "    w_output_param = None\n",
    "    if w_output:\n",
    "        w_output_param = torch.empty(n_actions)\n",
    "        nn.init.normal_(w_output_param, mean=90.)\n",
    "\n",
    "    return {\n",
    "        \"n_qubits\": n_qubits,\n",
    "        \"n_actions\": n_actions,\n",
    "        \"data_reupload\": data_reupload,\n",
    "        \"q_layers\": q_layers,\n",
    "        \"w_input\": w_input_param,\n",
    "        \"w_output\": w_output_param\n",
    "    }\n",
    "\n",
    "# Implementa la operación \"forward\" para el modelo QuantumNet\n",
    "def quantum_net_forward(quantum_net, inputs):\n",
    "    # Aplica el peso de entrada si está definido\n",
    "    if quantum_net[\"w_input\"] is not None:\n",
    "        inputs = inputs * quantum_net[\"w_input\"]\n",
    "\n",
    "    # Aplica la función no lineal atan\n",
    "    inputs = torch.atan(inputs)\n",
    "\n",
    "    # Evalúa el circuito cuántico para cada entrada en el batch\n",
    "    outputs = torch.stack([quantum_net[\"q_layers\"](input) for input in inputs])\n",
    "\n",
    "    # Ajusta los valores de salida al rango [0, 1]\n",
    "    outputs = (1 + outputs) / 2\n",
    "\n",
    "    # Aplica el peso de salida si está definido, o multiplica por un valor predeterminado\n",
    "    if quantum_net[\"w_output\"] is not None:\n",
    "        outputs = outputs * quantum_net[\"w_output\"]\n",
    "    else:\n",
    "        outputs = 90 * outputs\n",
    "\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "acb1805e-2c78-4bac-afcd-93408ca0ff54",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_layers = 5\n",
    "gamma = 0.99\n",
    "w_input = True\n",
    "w_output = True\n",
    "lr = 0.001\n",
    "lr_input = 0.01\n",
    "lr_output = 0.01\n",
    "batch_size = 16\n",
    "eps_init = 1.\n",
    "eps_decay = 0.99\n",
    "eps_min = 0.01\n",
    "train_freq = 10\n",
    "target_freq = 30\n",
    "memory = 10000\n",
    "data_reupload = True\n",
    "loss = 'SmoothL1'\n",
    "optimizer = 'RMSprop'\n",
    "total_episodes = 5000\n",
    "n_eval_episodes = 5\n",
    "logging = True\n",
    "log_train_freq = 1\n",
    "log_eval_freq = 20\n",
    "log_ckp_freq = 20\n",
    "device = 'cpu'  # 'auto', 'cpu', 'cuda:0'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2f0335-00a0-4b3c-9acb-9695971cc84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importa librerías necesarias\n",
    "import gym\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# Define el entorno\n",
    "env_name = 'CartPole-v1'  # Cambia a CartPole-v1\n",
    "env = gym.make(env_name)\n",
    "\n",
    "# Inicializa las redes neuronales cuánticas\n",
    "net = init_quantum_net(n_layers, w_input, w_output, data_reupload)\n",
    "target_net = init_quantum_net(n_layers, w_input, w_output, data_reupload)\n",
    "\n",
    "# Inicializa el entrenador\n",
    "trainer = init_trainer(\n",
    "    env=env,\n",
    "    net=net[\"q_layers\"],         # Modelo cuántico principal\n",
    "    target_net=target_net[\"q_layers\"],  # Modelo cuántico objetivo\n",
    "    gamma=gamma,\n",
    "    learning_rate=lr,\n",
    "    batch_size=batch_size,\n",
    "    exploration_initial_eps=eps_init,\n",
    "    exploration_decay=eps_decay,\n",
    "    exploration_final_eps=eps_min,\n",
    "    train_freq=train_freq,\n",
    "    target_update_interval=target_freq,\n",
    "    buffer_size=memory,\n",
    "    learning_rate_input=lr_input,\n",
    "    learning_rate_output=lr_output,\n",
    "    loss_func=loss,\n",
    "    optim_class=optimizer,\n",
    "    device=device,\n",
    "    logging=logging\n",
    ")\n",
    "\n",
    "# Reinicia el entrenador para inicializar la memoria\n",
    "reset_trainer(trainer)\n",
    "\n",
    "# Configuración de TensorBoard\n",
    "from datetime import datetime\n",
    "log_dir = f\"logs/{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}/\"\n",
    "writer = SummaryWriter(log_dir=log_dir)\n",
    "print(f\"TensorBoard logs stored at: {log_dir}\")\n",
    "\n",
    "# Entrena el modelo\n",
    "train(\n",
    "    trainer=trainer,\n",
    "    total_episodes=total_episodes,\n",
    "    n_eval_episodes=n_eval_episodes,\n",
    "    log_train_freq=log_train_freq,\n",
    "    log_eval_freq=log_eval_freq,\n",
    "    log_ckp_freq=log_ckp_freq\n",
    ")\n",
    "\n",
    "# Lanza TensorBoard\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir=logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c185a0d-2115-4cd8-b9f0-b7654ba70921",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
